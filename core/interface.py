from datetime import datetime
from json import dumps
from queue import Queue
from time import sleep
from core.cve import CVESearch
from core.github.repository import Repository
from core.github.parsers.python.package_resolver import resolve_best_version_url
from core.util import download_and_unpack_tar
from core.data_store import data_store
from core.versioning import pack_version_expression


class CountedQueue:
    def __init__(self):
        self.queue = Queue()
        self.processed = self.total = 0

    def put(self, item):
        self.queue.put(item)
        self.total += 1

    def get(self):
        self.processed += 1
        return self.queue.get()

    def empty(self):
        return self.total == self.processed


class DataInterface:
    def __init__(self):
        self.cve_search = CVESearch()
        self._cache = {}

    def get_cached(self, url):
        if url in self._cache:
            return self._cache.get(url).copy()

    def stream_process_repository(self, url):
        yield {"status": "Queued for processing."}
        sleep(0.1)  # todo: implement a queue for spikes in traffic?

        processing_queue = CountedQueue()
        processing_queue.put({"type": "repository", "url": url})
        dependencies = []
        seen = set()
        dependency_graph = []

        while not processing_queue.empty():
            item = processing_queue.get()
            if item["type"] == "repository":
                url = item["url"]
                for status in self.process_repository(url, dependencies, processing_queue, dependency_graph, seen):
                    yield {"status": f"Processing ({processing_queue.processed}/{processing_queue.total})... {status}"}

            elif item["type"] == "python.package":
                name, version = item["name"], item["version"]
                if name in seen:
                    continue
                seen.add(name)
                for status in self.process_python_package(
                    name, version, dependencies, processing_queue, dependency_graph, seen
                ):
                    yield {"status": f"Processing ({processing_queue.processed}/{processing_queue.total})... {status}"}

        vertices = [{"name": url, "type": "repository", "version": "", "known_vulnerabilities": []}]
        vertices.extend({
            "name": dep["name"], "type": dep["type"], "version": pack_version_expression(dep["version"]),
            "known_vulnerabilities": dep["known_vulnerabilities"]
        } for dep in dependencies)

        self._cache[url] = {
            "vertices": vertices,
            "edges": dependency_graph,
            "meta": {
                "repository_last_scanned": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "known_vulnerabilities_last_updated":
                    self.cve_search.known_vulnerabilities_updated.strftime("%Y-%m-%d %H:%M:%S"),
                "index_completeness": self.cve_search.cve_index_completeness
            }
        }

        yield dumps({"status": "Done."})+"\n"

    def process_repository(self, url, dependencies, processing_queue, dependency_graph, seen):
        yield f"Cloning {url}..."
        repository = Repository.from_url(url)
        repository.clone()

        yield f"Scanning {url} for vulnerabilities..."
        self.process_repository_object(repository, dependencies, processing_queue, dependency_graph, seen)

    def process_python_package(self, package, version, dependencies, processing_queue, dependency_graph, seen):
        yield f"Resolving {package}..."
        best_version, url = resolve_best_version_url(package, version)
        if not best_version:
            return  # did not resolve, false positive?

        cached = data_store.lookup_dependencies("python", package, best_version)
        if cached is not None:
            self.include_dependencies(package, cached, dependencies, processing_queue, dependency_graph, seen)
            return

        yield f"Downloading {package}..."
        output = download_and_unpack_tar(url)

        yield f"Scanning {package} for vulnerabilities..."
        repository = Repository.from_path(package, output)
        to_store = self.process_repository_object(repository, dependencies, processing_queue, dependency_graph, seen)
        data_store.store_dependencies("python", package, best_version, to_store)

    def process_repository_object(self, repository, dependencies, processing_queue, dependency_graph, seen):
        try:
            new = list(repository.parse_direct_dependencies())
            self.include_dependencies(repository.name, new, dependencies, processing_queue, dependency_graph, seen)
            return new
        finally:
            repository.cleanup()

    def include_dependencies(self, root, new_dependencies, dependencies, processing_queue, dependency_graph, seen):
        for dependency in new_dependencies:
            # fixme: we may be missing vulnerabilities here
            # if there are multiple conflicting version constraints active at the same time
            if dependency["name"] in seen:
                continue

            vulnerabilities = list(set(self.cve_search.search(dependency["name"], dependency["version"])))
            dependencies.append({**dependency, "known_vulnerabilities": vulnerabilities})
            processing_queue.put(dependency)
            dependency_graph.append((root, dependency["name"]))
